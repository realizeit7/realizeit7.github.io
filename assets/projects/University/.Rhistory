for (i in 1:trial)
{
for (j in 1:ncol){
stim[i,j]<-rbinom(1,1,0.5)
}
}
diff<-matrix(rep(c(rep(1,ncol*48),rep(0,ncol*48)),11),nrow = trial, ncol = ncol,byrow=TRUE)
cond<-matrix(rep(c(rep(1,ncol*24),rep(0,ncol*24)),22), nrow = trial, ncol = ncol,byrow=TRUE)
resp[1,]<-rbinom(ncol,1,0.5)
#mu0=-1 mu1=1 k=0 d1=0.7 p=0.4 d2=1.8
for (j in 1:ncol){
for (i in 2:trial){
rho = 1/(1 + exp(-1 + cond[i,j]*1));
x=  -stim[i,j]*0.7*(1-diff[i,j])-stim[i,j]*1.8*(diff[i,j]);
pq = 1 - 0.4 + resp[i-1,j]*(2*0.4-1)
ps = rho*(1-pnorm(x/2,0,1)) + (1-rho)*pq
resp[i,j]=rbinom(1,1,ps)
}
}
library(rstan)
model.script="
data {
// Variables passed to Stan from R
int<lower=0> N; // Number of subjects
int<lower=0> M; // Number of trials
int<lower=0,upper=1> resp[M,N]; // Responses
int<lower=0,upper=1> stim[M,N]; // Stimuli
int<lower=0,upper=1> cond[M,N]; // Condition
int<lower=0,upper=1> diff[M,N]; // difficulty
}
parameters {
// Declaration of untransformed individual-level parameters
//
// Standardized SDT parameters
real zd1[N];            // d-prime
real zk1[N];            // bias
real zd2[N];            // d-prime2
real zk2[N];            // bias2
// Markov chain parameters
// real<lower=0,upper=1> rho[N]; // mixture proportion
real zlp[N];   // P(0,0) (one-step transition probability)
real zlq[N];   // P(1,0) (one-step transition probability)
real zrho0[N]; // Mixing probability
real zrho1[N]; // Condition effect
// Hyperparameters
// SDT parameters
real mud1;             // D' mean
real mud2;
// Markov chain parameters
real mup;   // First one-step transition probability mean
real muq;   // Second one-step transition probability mean
real mu0; // Mixing proportion mean
real mu1; // Condition effect
}
transformed parameters {
// Compute the parameters for the logit distribution
// Markov chain parameter declarations
real p[N];
// real q[N];
real rho0[N];
real rho1[N];
real d1[N];
real d2[N];
for (i in 1:N) {
// First step transition and mixing probabilities
p[i] = 1/(1 + exp(zlp[i] + mup));
//       q[i] = 1/(1 + exp(zlq[i] + muq));
// Transformed normal parameters
d1[i] = zd1[i] + mud1;
d2[i] = zd2[i] + mud2;
rho0[i] = zrho0[i] + mu0;
rho1[i] = zrho1[i] + mu1;
}
}
model {
// Sample from the mixture model for each observation in r and s.
//
// Local variable declarations
real x;   // Z-score of the criterion k for noise and signal trials
real pq;  // The one-step transition probability
real ps;  // The overall response probability for the response
real rho; // The mixture probability
real q[N]; // q fixed at 1 - p
// Hyperprior sampling (see graphical model)
//
// Sample the normal hyperpriors for bias k and d'
mud1 ~ normal(0,1);
mud2 ~ normal(1,1);
// Sample the logit mean for the first
// transition probability p
mup ~ normal(0,1);
// Sample the logit mean for the second
// transition probability q
muq ~ normal(0,1);
// Sample the logit mean for the mixture
// probability rho
mu0 ~ normal(0,1);
mu1 ~ normal(0,1);
// Individual-level parameter sampling
//
for (n in 1:N) {
// Sample standardized bias k and d' from the
// normal distribution
zd1[n] ~ normal(0,1);
zd2[n] ~ normal(0,1);
// Sample the one-step transition probabilities p and q
//     p = Pr(repeat a 0 response)
// 1 - q = Pr(repeat a 1 response)
zlp[n] ~ normal(0,1);
// zlq[n] ~ normal(0,1);
// Sample the mixture probability
zrho0[n] ~ normal(0,1);
zrho1[n] ~ normal(0,1);
// For each observation, compute the likelihood
for (i in 2:M) {
// Response probability for Markov chain
pq = 1 - p[n] + resp[i-1,n]*(2*p[n]-1);
// Mixture probability
rho = 1/(1 + exp(rho0[n] + cond[i,n]*rho1[n]));
// Z-score of criterion k
x=  -stim[i,n]*d1[n]*(1-diff[i,n])-stim[i,n]*d2[n]*(diff[i,n]); //0 easy 1 hard
// Mixture response probability
ps= rho*(1-Phi(x/2)) + (1-rho)*pq;
// Compute likelihood
resp[i,n] ~ bernoulli(ps);
}
}
}
"
# Step 3
brn <- 300# Number of samples to initially discard
n.iter <- 3000 # Number of samples to draw
# Step 4
dat<-list(N=ncol,M=trial,resp=resp,stim=stim,cond=cond)
# Stan converts script into C++ code, compiles, and then samples
fit <- stan(model_code =model.script, data = dat,
iter = n.iter+brn, warmup=brn, chains = 3)
# Brief summary of results
# Step 5
# Extract chains for use in R
post <- extract(fit) # Creates a list with the output from Stan
save.image("PRecover_Study2_March_two_difficulty.RData")
plot(post$mud1)
plot(post$mud1,type="1")
plot(post$mud1,type="l")
abline(h=0.7,col="red")
plot(post$mud2,type="l")
abline(h=1.8,col="red")
plot(post$mu0,type="l")
abline(h=-1,col="red")
plot(post$mu1,type="l")
abline(h=1,col="red")
plot(1/(1+exp(post$mup)),type="l")
abline(h=0.4,col="red")
plot(1/(1+exp(post$mu1)),type="l")
plot(1/(1+exp(post$mu1)),type="l")
abline(h=1/(1+exp(-1)),col="red")
abline(h=1/(1+exp(-1)),col="red")
abline(h=1/(1+exp(-1),col="red")
1/(1+exp(-1))
abline(h=1/(1+exp(1),col="red")
abline(h=1/(1+exp(1)),col="red")
plot(post$mud1,type="l")
abline(h=0.7,col="red")
plot(post$mud2,type="l")
abline(h=1.8,col="red")
plot(post$mu0,type="l")
abline(h=-1,col="red")
plot(post$mu1,type="l")
abline(h=1,col="red")
plot(1/(1+exp(post$mup)),type="l")
abline(h=0.4,col="red")
plot(1/(1+exp(post$mu1)),type="l")
plot(1/(1+exp(post$mu1)),type="l")
abline(h=1/(1+exp(+1)),col="red")
plot(post$mud1,type="l")
abline(h=0.7,col="red")
plot(post$mud2,type="l")
abline(h=1.8,col="red")
plot(post$mu0,type="l")
abline(h=-1,col="red")
plot(post$mu1,type="l")
abline(h=1,col="red")
plot(1/(1+exp(post$mup)),type="l")
abline(h=0.4,col="red")
plot(1/(1+exp(post$mu1)),type="l")
abline(h=1/(1+exp(+1)),col="red")
plot(post$mud1,type="l")
abline(h=0.7,col="red")
plot(post$mud2,type="l")
abline(h=1.8,col="red")
plot(1/(1+exp(post$mu0)),type="l")
abline(h=-1,col="red")
plot(post$mu1,type="l")
abline(h=1,col="red")
plot(1/(1+exp(post$mup)),type="l")
abline(h=0.4,col="red")
plot(1/(1+exp(post$mu1)),type="l")
abline(h=1/(1+exp(+1)),col="red")
plot(1/(1+exp(post$mu0+post$mu1)),type="l")
abline(h=1/(1+exp(0)),col="red")
plot(1/(1+exp(post$mu0)),type="l")
abline(h=-1,col="red")
plot(post$mud1,type="l")
abline(h=0.7,col="red")
plot(post$mud2,type="l")
abline(h=1.8,col="red")
plot(1/(1+exp(post$mu0)),type="l")
abline(h=1/(1+exp(-1), col="red"))
plot(post$mu1,type="l")
abline(h=1,col="red")
plot(post$mud1,type="l")
abline(h=0.7,col="red")
plot(post$mud2,type="l")
abline(h=1.8,col="red")
plot(1/(1+exp(post$mu0)),type="l")
abline(h=1/(1+exp(-1)), col="red")
plot(post$mu1,type="l")
abline(h=1,col="red")
plot(1/(1+exp(post$mup)),type="l")
abline(h=0.4,col="red")
plot(1/(1+exp(post$mu0+post$mu1)),type="l")
abline(h=1/(1+exp(0)),col="red")
plot(post$mu0+post$mu1,type="l")
abline(h=1/(1+exp(0)),col="red")
plot(post$mu0+post$mu1,type="l")
abline(h=0,col="red")
plot(post$mu0+post$mu1,type="l")
abline(h=0.4,col="red")
plot(1/(1+exp(post$mup)),type="l")
abline(h=0.4,col="red")
plot(post$mud1,type="l")
abline(h=0.7,col="red")
plot(post$mud2,type="l")
abline(h=1.8,col="red")
plot(1/(1+exp(post$mu0)),type="l")
abline(h=1/(1+exp(-1)), col="red")
plot(post$mu1,type="l")
abline(h=1,col="red")
plot(1/(1+exp(post$mup)),type="l")
abline(h=0.4,col="red")
plot(post$mu0+post$mu1,type="l")
abline(h=0,col="red")
plot(post$mud1,type="l")
abline(h=0.7,col="red")
plot(post$mud2,type="l")
abline(h=1.8,col="red")
plot(1/(1+exp(post$mu0)),type="l")
abline(h=1/(1+exp(-1)), col="red")
plot(post$mu1,type="l")
abline(h=1,col="red")
plot(1/(1+exp(post$mup)),type="l")
plot(post$mu0+post$mu1,type="l")
abline(h=0,col="red")
abline(h=1/(1+exp(-1)), col="red")
plot(post$mu1,type="l")
abline(h=1,col="red")
plot(1/(1+exp(post$mup)),type="l")
abline(h=0.4,col="red")
plot(post$mu0+post$mu1,type="l")
abline(h=0,col="red")
plot(post$mu0,type="l")
abline(h=-1, col="red")
plot(post$mu1,type="l")
abline(h=1,col="red")
plot(1/(1+exp(post$mup)),type="l")
abline(h=0.4,col="red")
plot(post$mu0+post$mu1,type="l")
abline(h=0,col="red")
plot(post$mu0+post$mu1,type="l")
abline(h=0,col="red")
global <- read.csv("~/Downloads/global.txt", sep="")
View(global)
load("~/Downloads/global.sav")
library(haven)
global <- read_sav("Downloads/global.sav")
View(global)
install.packages("rtdists")
library(rtdists)
devtools::install_github("jason-morgan/ina")
data("Strike")
install.packages(c("backports", "broom", "callr", "colorspace", "curl", "dbplyr", "elliptic", "evaluate", "forcats", "forge", "fs", "git2r", "gsl", "gtable", "haven", "highr", "httpuv", "httr", "igraph", "jsonlite", "knitr", "later", "lazyeval", "lme4", "loo", "MASS", "Matrix", "mgcv", "modelr", "network", "neurobase", "openssl", "pbapply", "permute", "pkgbuild", "processx", "ps", "R.utils", "r2d3", "RcppArmadillo", "RcppEigen", "readxl", "reticulate", "rlang", "rmarkdown", "RNifti", "rstudioapi", "shiny", "sparklyr", "spm12r", "StanHeaders", "stringi", "stringr", "survival", "tidyr", "tinytex", "xfun"))
install.packages(c("backports", "broom", "callr", "colorspace", "curl", "dbplyr", "elliptic", "evaluate", "forcats", "forge", "fs", "git2r", "gsl", "gtable", "haven", "highr", "httpuv", "httr", "igraph", "jsonlite", "knitr", "later", "lazyeval", "lme4", "loo", "MASS", "Matrix", "mgcv", "modelr", "network", "neurobase", "openssl", "pbapply", "permute", "pkgbuild", "processx", "ps", "R.utils", "r2d3", "RcppArmadillo", "RcppEigen", "readxl", "reticulate", "rlang", "rmarkdown", "RNifti", "rstudioapi", "shiny", "sparklyr", "spm12r", "StanHeaders", "stringi", "stringr", "survival", "tidyr", "tinytex", "xfun"))
install.packages(c("backports", "broom", "callr", "colorspace", "curl", "dbplyr", "elliptic", "evaluate", "forcats", "forge", "fs", "git2r", "gsl", "gtable", "haven", "highr", "httpuv", "httr", "igraph", "jsonlite", "knitr", "later", "lazyeval", "lme4", "loo", "MASS", "Matrix", "mgcv", "modelr", "network", "neurobase", "openssl", "pbapply", "permute", "pkgbuild", "processx", "ps", "R.utils", "r2d3", "RcppArmadillo", "RcppEigen", "readxl", "reticulate", "rlang", "rmarkdown", "RNifti", "rstudioapi", "shiny", "sparklyr", "spm12r", "StanHeaders", "stringi", "stringr", "survival", "tidyr", "tinytex", "xfun"))
install.packages(c("backports", "broom", "callr", "colorspace", "curl", "dbplyr", "elliptic", "evaluate", "forcats", "forge", "fs", "git2r", "gsl", "gtable", "haven", "highr", "httpuv", "httr", "igraph", "jsonlite", "knitr", "later", "lazyeval", "lme4", "loo", "MASS", "Matrix", "mgcv", "modelr", "network", "neurobase", "openssl", "pbapply", "permute", "pkgbuild", "processx", "ps", "R.utils", "r2d3", "RcppArmadillo", "RcppEigen", "readxl", "reticulate", "rlang", "rmarkdown", "RNifti", "rstudioapi", "shiny", "sparklyr", "spm12r", "StanHeaders", "stringi", "stringr", "survival", "tidyr", "tinytex", "xfun"))
load("~/Dropbox/Master_project_re/PRecover_Study2_april_three_difficulty.RData")
load("~/Dropbox/Master_project_re/PRecover_Study2_april_three_difficulty.RData")
post$mud1
hist(post$mud1)
hist(post$mud2)
hist(post$mud3)
diff
load("/Users/seowookchoi/Library/Containers/com.apple.mail/Data/Library/Mail Downloads/125C664A-3EDA-4BC0-BD24-8B05862ED5FE/entity_image.RData")
View(unlabel)
setwd("~/Downloads")
write(unlabel,file="unlabeled.csv",row.names=FALSE)
write.csv(unlabel,file="unlabeled.csv",row.names=FALSE)
setwd("~/Desktop/entity")
setwd("~/Desktop/entity/University")
read.csv('unlabeled_predictions.csv')
unlabeled_predictions=read.csv('unlabeled_predictions.csv')
correct=subset(unlabeled_predictions,unlabeled_predictions$match_score>0.4)
View(correct)
correct=subset(unlabeled_predictions,unlabeled_predictions$match_score>0.8)
dim(correct)
correct=subset(unlabeled_predictions,unlabeled_predictions$match_score>0.9)
dim(correct)
View(correct)
head(correct)
newdata <-correct[order(left_School_Name),]
newdata <-correct[order(correct$left_School_Name),]
View(newdata)
correct=subset(unlabeled_predictions,unlabeled_predictions$match_score>0.95)
new=correct[
with(correct, order(left_School_Name, match_score)),
]
new
dim(new)
View(new)
library(tidyverse)
new=arrange(correct, desc(left_School_Name),desc(match_score))
new
View(new)
?arrange
new=arrange(correct, left_School_Name,desc(match_score))
new
View(new)
write.csv(match_score_over_0.95,'match_score_over_0.95',row.names = FALSE)
match_score_over_0.95=arrange(correct, left_School_Name,desc(match_score))
write.csv(match_score_over_0.95,'match_score_over_0.95',row.names = FALSE)
write.csv(match_score_over_0.95,'match_score_over_0.95.csv',row.names = FALSE)
View(new)
load("~/Desktop/entity/df.RData")
df0<-subset(df[sample(nrow(df)),],!duplicated(left_School_Name))
# Table of matches
df2 <- df %>%
mutate(label = 1) %>%
select(label, left_School_Name, right_School_Name)
library(foreach)
newdf =df
newdf$label = 1
newdf$train = sample(c(0,1,2),size=nrow(newdf),replace=TRUE,prob=c(0.2,0.2,0.6))
newdf_add= foreach(i = 1:nrow(newdf),.combine=rbind) %dopar% {
library(dplyr)
message(i)
schools = unique(filter(newdf,right_School_Name != newdf$right_School_Name[i])$right_School_Name)
newids = sample(schools,100,replace=FALSE)
add = data.frame(right_School_Name =  newids)
add$left_School_Name= newdf$left_School_Name[i]
add$train= newdf$train[i]
add$label = 0
add = select_at(add,names(newdf))
return(add)
}
newdf = rbind(newdf,newdf_add)
# Table of non-matches
df3 <- df2 %>%
mutate(label = 0) %>%
select(label, left_School_Name, right_School_Name)
# In the table of non-matches, randomly order right school names. This ensures
# that what we're calling non-matches (label == 0) are indeed (mostly)
# non-matches.
set.seed(1)
df3$right_School_Name <- df3$right_School_Name[sample(1:length(df3$right_School_Name))]
# Combine random 1K samples from the match and non-match tables; add in id col
df4 <- rbind(df2 %>% sample_n(10000), df3 %>% sample_n(10000)) %>%
mutate(id = row_number()) %>%
select(id, label, left_School_Name, right_School_Name)
# Create training set
train.index <- createDataPartition(df4$label, p = 0.95, list = FALSE)
train <- df4[train.index,]
# Non-training rows go into this temp table, which is then subdivided into test
# and validation sets.
temp <- df4[-train.index,]
test.index <- createDataPartition(temp$label, p = .5, list = FALSE)
test <- temp[test.index,]
validation <- temp[-test.index,]
# Write to file
write_csv(train, "school_data/train.csv")
write_csv(test, "school_data/test.csv")
write_csv(validation, "school_data/validation.csv")
SQLcode="select school_name_location, s.school_conformed_name, s.prod_sf_picklist_school_id
from hrdw.d_schools s
where school_conformed_name != '' and prod_sf_picklist_school_id is not null
order by school_conformed_name" # Query
library(keras)
library(psych)# for dummy code
library(tidyverse) # for pipe
library(stringi) # for cleaning wierd characters
library(stringr)
library(HRWFIDB)
DB=HRWFIDB::ExecSQL(SQLcode) # one x variable 2 y variables (actually 1)
DB$school_name_location <-stri_trans_general(DB$school_name_location,'Latin-ASCII')
# erasing wierd characters
rows <- sample(nrow(DB)) #shuffling data order
DB <- DB[rows, ]       #shuffling data order
#word level
#library(keras)
samples <-DB$school_name_location
#####data preprocessin for unidentifable characters
samples <- stringr::str_replace_all(samples, "[[:punct:]]", " ")
samples<-str_replace_all(samples,'[^[:alpha:]]','')
right_1<-unique(DB$school_conformed_name)
unlabel=expand.grid(test$left_School_Name,right_1)
unlabel=cbind(1:dim(unlabel)[1],unlabel)
names(unlabel)<-c("id","left_School_Name","right_School_Name")
write.csv(unlabel,file="unlabeled.csv",row.names=FALSE)
# Packages
library(tidyverse)
library(caret)
install.packages("caret")
df0<-subset(df[sample(nrow(df)),],!duplicated(left_School_Name))
# Table of matches
df2 <- df %>%
mutate(label = 1) %>%
select(label, left_School_Name, right_School_Name)
library(foreach)
newdf =df
newdf$label = 1
newdf$train = sample(c(0,1,2),size=nrow(newdf),replace=TRUE,prob=c(0.2,0.2,0.6))
newdf_add= foreach(i = 1:nrow(newdf),.combine=rbind) %dopar% {
library(dplyr)
message(i)
schools = unique(filter(newdf,right_School_Name != newdf$right_School_Name[i])$right_School_Name)
newids = sample(schools,100,replace=FALSE)
add = data.frame(right_School_Name =  newids)
add$left_School_Name= newdf$left_School_Name[i]
add$train= newdf$train[i]
add$label = 0
add = select_at(add,names(newdf))
return(add)
}
newdf = rbind(newdf,newdf_add)
newdf_add= foreach(i = 1:nrow(newdf),.combine=rbind) %dopar% {
library(dplyr)
message(i)
schools = unique(filter(newdf,right_School_Name != newdf$right_School_Name[i])$right_School_Name)
newids = sample(schools,100,replace=FALSE)
add = data.frame(right_School_Name =  newids)
add$left_School_Name= newdf$left_School_Name[i]
add$train= newdf$train[i]
add$label = 0
add = select_at(add,names(newdf))
return(add)
}
nrow(newdf)
newdf_add= foreach(i = 1:nrow(newdf),.combine=rbind) %do% {
library(dplyr)
message(i)
schools = unique(filter(newdf,right_School_Name != newdf$right_School_Name[i])$right_School_Name)
newids = sample(schools,100,replace=FALSE)
add = data.frame(right_School_Name =  newids)
add$left_School_Name= newdf$left_School_Name[i]
add$train= newdf$train[i]
add$label = 0
add = select_at(add,names(newdf))
return(add)
}
newdf_add= foreach(i = 1:nrow(newdf),.combine=rbind) %dopar% {
library(dplyr)
message(i)
schools = unique(filter(newdf,right_School_Name != newdf$right_School_Name[i])$right_School_Name)
newids = sample(schools,100,replace=FALSE)
add = data.frame(right_School_Name =  newids)
add$left_School_Name= newdf$left_School_Name[i]
add$train= newdf$train[i]
add$label = 0
add = select_at(add,names(newdf))
return(add)
}
test <- read.csv("~/Desktop/entity/University/test.csv")
View(test)
test[,1:3]
train <- read.csv("~/Desktop/entity/University/train.csv")
View(train)
validation <- read.csv("~/Desktop/entity/University/validation.csv")
View(validation)
head(train)
train[,3:1]
train[,c(3,1,2)]
head(train)
train[,c(3,1,2)]
head(train)
write_csv(new_train, "train.csv")
new_train<-train[,c(3,1,2)]
new_test<-test[,c(3,1,2)]
new_validation<-validation[,c(3,1,2)]
write_csv(new_train, "train.csv")
write_csv(new_test, "test.csv")
write_csv(new_validation, "validation.csv")
new_train$id<-1
new_train$id
dim(train)
new_train<-train[,c(3,1,2)]
new_train$id<-1:dim(train)[1]
new_test<-test[,c(3,1,2)]
new_test$id<-1:dim(test)[1]
new_validation<-validation[,c(3,1,2)]
new_validation$id<-1:dim(validation)[1]
write_csv(new_train, "train.csv")
write_csv(new_test, "test.csv")
write_csv(new_validation, "validation.csv")
unlabeled_predictions <- read.csv("~/Desktop/entity/University/unlabeled_predictions.csv")
View(unlabeled_predictions)
?sort
new<-unlabeled_predictions [order(-match_score),]
View(new)
new
new<-unlabeled_predictions [order(-match_score),]
unlabeled_predictions$match_score
new2<-unlabeled_predictions [order(-unlabeled_predictions$match_score),]
View(new2)
